{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "    '''\n",
    "    Must extend the torch.autograd.Function and have forward and backward\n",
    "    methods\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 64\n",
    "IN = 1000\n",
    "HIDDEN = 100\n",
    "OUT = 10\n",
    "\n",
    "ETA = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(BATCH, IN, device=device, dtype=dtype)\n",
    "y = torch.randn(BATCH, OUT, device=device, dtype=dtype)\n",
    "\n",
    "W1 = torch.randn(\n",
    "    IN, HIDDEN, device=device, dtype=dtype, requires_grad=True)\n",
    "W2 = torch.randn(\n",
    "    HIDDEN, OUT, device=device, dtype=dtype, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33060372.0\n",
      "1 5842550.0\n",
      "2 5492347.5\n",
      "3 5479184.0\n",
      "4 5698065.0\n",
      "5 6042964.0\n",
      "6 6423330.5\n",
      "7 6717053.0\n",
      "8 6838993.0\n",
      "9 6696388.5\n",
      "10 6295803.0\n",
      "11 5647687.0\n",
      "12 4858834.0\n",
      "13 4013733.0\n",
      "14 3213731.5\n",
      "15 2506150.0\n",
      "16 1921395.25\n",
      "17 1455071.625\n",
      "18 1097206.75\n",
      "19 826463.8125\n",
      "20 625172.6875\n",
      "21 476058.46875\n",
      "22 366137.53125\n",
      "23 284869.0\n",
      "24 224606.140625\n",
      "25 179552.1875\n",
      "26 145663.8125\n",
      "27 119888.0625\n",
      "28 100064.1875\n",
      "29 84626.6171875\n",
      "30 72446.2109375\n",
      "31 62695.33984375\n",
      "32 54787.9453125\n",
      "33 48289.1328125\n",
      "34 42874.04296875\n",
      "35 38311.34375\n",
      "36 34420.77734375\n",
      "37 31068.646484375\n",
      "38 28153.607421875\n",
      "39 25599.015625\n",
      "40 23343.779296875\n",
      "41 21341.0859375\n",
      "42 19552.27734375\n",
      "43 17947.69921875\n",
      "44 16502.865234375\n",
      "45 15196.126953125\n",
      "46 14011.6015625\n",
      "47 12934.1005859375\n",
      "48 11952.53125\n",
      "49 11056.7607421875\n",
      "50 10237.90625\n",
      "51 9487.759765625\n",
      "52 8799.15625\n",
      "53 8166.677734375\n",
      "54 7585.46142578125\n",
      "55 7049.75634765625\n",
      "56 6555.984375\n",
      "57 6100.12158203125\n",
      "58 5679.1337890625\n",
      "59 5290.06640625\n",
      "60 4929.94287109375\n",
      "61 4596.6533203125\n",
      "62 4287.7900390625\n",
      "63 4001.46533203125\n",
      "64 3736.177734375\n",
      "65 3489.626220703125\n",
      "66 3260.747802734375\n",
      "67 3047.901123046875\n",
      "68 2850.0068359375\n",
      "69 2665.97119140625\n",
      "70 2494.620361328125\n",
      "71 2335.10498046875\n",
      "72 2186.45458984375\n",
      "73 2047.906982421875\n",
      "74 1918.7557373046875\n",
      "75 1798.2672119140625\n",
      "76 1685.7957763671875\n",
      "77 1580.7686767578125\n",
      "78 1482.68310546875\n",
      "79 1391.1275634765625\n",
      "80 1305.4652099609375\n",
      "81 1225.3704833984375\n",
      "82 1150.4693603515625\n",
      "83 1080.3924560546875\n",
      "84 1014.8115844726562\n",
      "85 953.462646484375\n",
      "86 896.1124877929688\n",
      "87 842.395263671875\n",
      "88 792.063720703125\n",
      "89 744.8818359375\n",
      "90 700.656982421875\n",
      "91 659.1900024414062\n",
      "92 620.2870483398438\n",
      "93 583.7905883789062\n",
      "94 549.558837890625\n",
      "95 517.4420776367188\n",
      "96 487.25750732421875\n",
      "97 458.8990173339844\n",
      "98 432.2658386230469\n",
      "99 407.24456787109375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    relu = MyReLU.apply\n",
    "    y_pred = relu(x.mm(W1)).mm(W2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(epoch, loss.item())\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        W1 -= ETA * W1.grad\n",
    "        W2 -= ETA * W2.grad\n",
    "        W1.grad.zero_()\n",
    "        W2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
