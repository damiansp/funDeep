{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import argparse\n",
    "import input_data\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tensorflow.python.ops import control_flow_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "DISPLAY_STEP = 1\n",
    "\n",
    "# Batch norm:\n",
    "EMA_DECAY = 0.9\n",
    "\n",
    "# Optimizer\n",
    "ETA = 0.01\n",
    "BETA1 = 0.9\n",
    "BETA2 = 0.999\n",
    "EPSILON = 1e-8\n",
    "\n",
    "# Architecture\n",
    "N_ENCODER_H1 = N_DECODER_H3 = 1000\n",
    "N_ENCODER_H2 = N_DECODER_H2 = 500\n",
    "N_ENCODER_H3 = N_DECODER_H1 = 250\n",
    "N_CODE = 125 # size of encoded layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer_batch_norm(x, n_out, is_training):\n",
    "    beta_init  = tf.constant_initializer(value=0., dtype=tf.float32)\n",
    "    gamma_init = tf.constant_initializer(value=1., dtype=tf.float32)\n",
    "    beta  = tf.get_variable('beta',  [n_out], initializer=beta_init)\n",
    "    gamma = tf.get_variable('gamma', [n_out], initializer=gamma_init)\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0], name='moments')\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=EMA_DECAY)\n",
    "    ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "    ema_mean, ema_var = ema.average(batch_mean), ema.average(batch_var)\n",
    "    \n",
    "    def mean_var_with_update():\n",
    "        with tf.control_dependencies([ema_apply_op]):\n",
    "            return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "        \n",
    "    mean, var = control_flow_ops.cond(\n",
    "        is_training, mean_var_with_update, lambda: (ema_mean, ema_var))\n",
    "    x_reshaped = tf.reshape(x, [-1, 1, 1, n_out])\n",
    "    normed = tf.nn.batch_norm_with_global_normalization(\n",
    "        x_reshaped, mean, var, beta, gamma, 1e-3, True)\n",
    "    return tf.reshape(normed, [-1, n_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected(input, weight_shape, bias_shape, is_training):\n",
    "    weight_init = tf.random_normal_initializer(\n",
    "        stddev=(1. / weight_shape[0]) ** 0.5)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable('W', weight_shape, initializer=weight_init)\n",
    "    b = tf.get_variable('b', bias_shape,   initializer=bias_init)\n",
    "    logits = tf.matmul(input, W) + b\n",
    "    return tf.nn.tanh(\n",
    "        layer_batch_norm(logits, weight_shape[1], is_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(x, n_code, is_training):\n",
    "    with tf.variable_scope('encoder'):\n",
    "        with tf.variable_scope('h1'):\n",
    "            h1 = fully_connected(\n",
    "                x, [784, N_ENCODER_H1], [N_ENCODER_H1], is_training)\n",
    "        with tf.variable_scope('h2'):\n",
    "            h2 = fully_connected(h1, \n",
    "                                 [N_ENCODER_H1, N_ENCODER_H2], \n",
    "                                 [N_ENCODER_H2], \n",
    "                                 is_training)\n",
    "        with tf.variable_scope('h3'):\n",
    "            h3 = fully_connected(h2,\n",
    "                                 [N_ENCODER_H2, N_ENCODER_H3],\n",
    "                                 [N_ENCODER_H3],\n",
    "                                 is_training)\n",
    "        with tf.variable_scope('code'):\n",
    "            code = fully_connected(\n",
    "                h3, [N_ENCODER_H3, n_code], [n_code], is_training)\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(code, n_code, is_training):\n",
    "    with tf.variable_scope('decoder'):\n",
    "        with tf.variable_scope('h1'):\n",
    "            h1 = fully_connected(code, \n",
    "                                 [n_code, N_DECODER_H1], \n",
    "                                 [N_DECODER_H1], \n",
    "                                 is_training)\n",
    "        with tf.variable_scope('h2'):\n",
    "            h2 = fully_connected(h1, \n",
    "                                 [N_DECODER_H1, N_DECODER_H2], \n",
    "                                 [N_DECODER_H2], \n",
    "                                 is_training)\n",
    "        with tf.variable_scope('h3'):\n",
    "            h3 = fully_connected(h2, \n",
    "                                 [N_DECODER_H2, N_DECODER_H3], \n",
    "                                 [N_DECODER_H3], \n",
    "                                 is_training)\n",
    "        with tf.variable_scope(\"output\"):\n",
    "            output = fully_connected(\n",
    "                h3, [N_DECODER_H3, 784], [784], is_training)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(output, x):\n",
    "    with tf.variable_scope('training'):\n",
    "        l2 = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(output, x)), 1))\n",
    "        train_loss = tf.reduce_mean(l2)\n",
    "        train_summary_op = tf.summary.scalar('train_cost', train_loss)\n",
    "        return train_loss, train_summary_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(cost, global_step):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=ETA, \n",
    "                                       beta1=BETA1, \n",
    "                                       beta2=BETA2, \n",
    "                                       epsilon=EPSILON, \n",
    "                                       use_locking=False, \n",
    "                                       name='Adam')\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_summary(label, tensor):\n",
    "    tensor_reshaped = tf.reshape(tensor, [-1, 28, 28, 1])\n",
    "    return tf.summary.image(label, tensor_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(output, x):\n",
    "    with tf.variable_scope('validation'):\n",
    "        in_im_op = image_summary('input_image', x)\n",
    "        out_im_op = image_summary('output_image', output)\n",
    "        l2 = tf.sqrt(tf.reduce_sum(\n",
    "            tf.square(\n",
    "                tf.subtract(output, x, name='val_diff')), \n",
    "            1))\n",
    "        val_loss = tf.reduce_mean(l2)\n",
    "        val_summary_op = tf.summary.scalar('val_cost', val_loss)\n",
    "        return val_loss, in_im_op, out_im_op, val_summary_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 Cost: 6.54849421 Valid. loss: 4.81077480\n",
      "Epoch: 0002 Cost: 4.51690872 Valid. loss: 4.12351942\n",
      "Epoch: 0003 Cost: 3.96013405 Valid. loss: 3.62583232\n",
      "Epoch: 0004 Cost: 3.55808474 Valid. loss: 3.32946634\n",
      "Epoch: 0005 Cost: 3.28801332 Valid. loss: 3.05751657\n",
      "Epoch: 0006 Cost: 3.10688807 Valid. loss: 2.92666411\n",
      "Epoch: 0007 Cost: 2.97580387 Valid. loss: 2.77790880\n",
      "Epoch: 0008 Cost: 2.87400215 Valid. loss: 2.70492435\n",
      "Epoch: 0009 Cost: 2.80418155 Valid. loss: 2.68389130\n",
      "Epoch: 0010 Cost: 2.73504546 Valid. loss: 2.54963827\n",
      "Epoch: 0011 Cost: 2.67495095 Valid. loss: 2.53508329\n",
      "Epoch: 0012 Cost: 2.62286801 Valid. loss: 2.45514178\n",
      "Epoch: 0013 Cost: 2.56989626 Valid. loss: 2.40769243\n",
      "Epoch: 0014 Cost: 2.53533129 Valid. loss: 2.35881948\n",
      "Epoch: 0015 Cost: 2.49438467 Valid. loss: 2.35260797\n",
      "Epoch: 0016 Cost: 2.46109661 Valid. loss: 2.29149103\n",
      "Epoch: 0017 Cost: 2.42657231 Valid. loss: 2.26814651\n",
      "Epoch: 0018 Cost: 2.41041783 Valid. loss: 2.24904466\n",
      "Epoch: 0019 Cost: 2.37967764 Valid. loss: 2.24372268\n",
      "Epoch: 0020 Cost: 2.36529544 Valid. loss: 2.22748852\n",
      "Epoch: 0021 Cost: 2.33788588 Valid. loss: 2.19639206\n",
      "Epoch: 0022 Cost: 2.31245231 Valid. loss: 2.15058398\n",
      "Epoch: 0023 Cost: 2.28884822 Valid. loss: 2.13249588\n",
      "Epoch: 0024 Cost: 2.26902102 Valid. loss: 2.10071373\n",
      "Epoch: 0025 Cost: 2.25662083 Valid. loss: 2.13181973\n",
      "Epoch: 0026 Cost: 2.24117616 Valid. loss: 2.08243084\n",
      "Epoch: 0027 Cost: 2.22480445 Valid. loss: 2.08079362\n",
      "Epoch: 0028 Cost: 2.22056551 Valid. loss: 2.08723402\n",
      "Epoch: 0029 Cost: 2.19458329 Valid. loss: 2.01747632\n",
      "Epoch: 0030 Cost: 2.18734111 Valid. loss: 2.04307961\n",
      "Epoch: 0031 Cost: 2.16980323 Valid. loss: 2.02242923\n",
      "Epoch: 0032 Cost: 2.16157699 Valid. loss: 2.01479435\n",
      "Epoch: 0033 Cost: 2.15570600 Valid. loss: 2.03445554\n",
      "Epoch: 0034 Cost: 2.13550333 Valid. loss: 1.98649001\n",
      "Epoch: 0035 Cost: 2.12585317 Valid. loss: 1.97033727\n",
      "Epoch: 0036 Cost: 2.11736322 Valid. loss: 1.96424103\n",
      "Epoch: 0037 Cost: 2.10981391 Valid. loss: 1.95010293\n",
      "Epoch: 0038 Cost: 2.09294992 Valid. loss: 1.93575346\n",
      "Epoch: 0039 Cost: 2.09263052 Valid. loss: 1.94381583\n",
      "Epoch: 0040 Cost: 2.08260639 Valid. loss: 1.95105004\n",
      "Epoch: 0041 Cost: 2.07652367 Valid. loss: 1.92754185\n",
      "Epoch: 0042 Cost: 2.06862950 Valid. loss: 1.91062212\n",
      "Epoch: 0043 Cost: 2.06071076 Valid. loss: 1.92468786\n",
      "Epoch: 0044 Cost: 2.05103667 Valid. loss: 1.90299785\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-bd0cf5f7371a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                 _, new_cost, train_summary = sess.run(\n\u001b[1;32m     29\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_summary_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                     feed_dict={x: batch_x, is_training: True})\n\u001b[0m\u001b[1;32m     31\u001b[0m                 train_writer.add_summary(train_summary, \n\u001b[1;32m     32\u001b[0m                                          sess.run(global_step))\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.variable_scope('autoencoder'):\n",
    "        x = tf.placeholder('float', [None, 784]) # images: 28 * 28 = 784\n",
    "        is_training = tf.placeholder(tf.bool)\n",
    "        code = encoder(x, N_CODE, is_training)\n",
    "        output = decoder(code, N_CODE, is_training)\n",
    "        cost, train_summary_op = loss(output, x)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = training(cost, global_step)\n",
    "        eval_op, in_im_op, out_im_op, val_summary_op = evaluate(output, x)\n",
    "        summary_op = tf.summary.merge_all()\n",
    "        saver = tf.train.Saver(max_to_keep=20)\n",
    "        sess = tf.Session()\n",
    "        train_writer = tf.summary.FileWriter(\n",
    "            'mnist_autoencoder_h=%d_logs/' %N_CODE, graph=sess.graph)\n",
    "        val_writer = tf.summary.FileWriter(\n",
    "            'mnist_autoencoder_h=%d_logs/' %N_CODE, graph=sess.graph)\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        # Train\n",
    "        for epoch in range(EPOCHS):\n",
    "            avg_cost = 0.\n",
    "            n_batches = mnist.train.num_examples // BATCH_SIZE\n",
    "            \n",
    "            for batch in range(n_batches):\n",
    "                batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\n",
    "                _, new_cost, train_summary = sess.run(\n",
    "                    [train_op, cost, train_summary_op], \n",
    "                    feed_dict={x: batch_x, is_training: True})\n",
    "                train_writer.add_summary(train_summary, \n",
    "                                         sess.run(global_step))\n",
    "                avg_cost += new_cost / n_batches\n",
    "                \n",
    "            if epoch % DISPLAY_STEP == 0:\n",
    "                train_writer.add_summary(train_summary, \n",
    "                                         sess.run(global_step))\n",
    "                validation_loss, in_im, out_im, val_summary = sess.run(\n",
    "                    [eval_op, in_im_op, out_im_op, val_summary_op],\n",
    "                    feed_dict={x: mnist.validation.images, \n",
    "                               is_training: False})\n",
    "                val_writer.add_summary(in_im,       sess.run(global_step))\n",
    "                val_writer.add_summary(out_im,      sess.run(global_step))\n",
    "                val_writer.add_summary(val_summary, sess.run(global_step))\n",
    "                print('Epoch: %04d Cost: %.8f Valid. loss: %.8f' \n",
    "                      %(epoch + 1, avg_cost, validation_loss))\n",
    "\n",
    "                saver.save(\n",
    "                    sess, \n",
    "                    'mnist_autoencoder_h=%d_logs/model-checkpoint-%04d'\n",
    "                    %(N_CODE, epoch + 1),\n",
    "                    global_step=global_step)\n",
    "                \n",
    "        print('Optimization finished!')\n",
    "        test_loss = sess.run(eval_op, \n",
    "                             feed_dict={x: mnist.test.images, \n",
    "                                        is_training: False})\n",
    "        print('Test Loss:', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
