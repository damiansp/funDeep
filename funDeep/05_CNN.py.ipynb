{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/darksigma/Fundamentals-of-Deep-Learning-Book\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./info.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03_nns_in_tf.py.ipynb            dropout_regularization.py\r\n",
      "04_beyond_gradient_descent.ipynb info.txt\r\n",
      "05_CNN.py.ipynb                  input_data.py\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m                      inverted_dropout.py\r\n",
      "\u001b[34mconv_mnist_logs\u001b[m\u001b[m                  \u001b[34mlogistic_logs\u001b[m\u001b[m\r\n",
      "\u001b[34mdata\u001b[m\u001b[m                             neuron.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import input_data\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Architecture\n",
    "n_hidden_1 = 256\n",
    "n_hidden_2 = 256\n",
    "\n",
    "# Hyperparameters\n",
    "ETA = 0.0001\n",
    "EPOCHS = 1000\n",
    "BATCH = 100\n",
    "DISPLAY_STEP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(input, weight_shape, bias_shape):\n",
    "    incoming = weight_shape[0] * weight_shape[1] * weight_shape[2]\n",
    "    weight_init = tf.random_normal_initializer(\n",
    "        stddev=(2. / incoming) ** 0.5)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable('W', weight_shape, initializer=weight_init)\n",
    "    b = tf.get_variable('b', bias_shape, initializer=bias_init)\n",
    "    return tf.nn.relu(tf.nn.bias_add(\n",
    "        tf.nn.conv2d(input, W, strides=[1, 1, 1, 1], padding='SAME'), b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(input, k=2):\n",
    "    return tf.nn.max_pool(\n",
    "        input, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer(input, weight_shape, bias_shape):\n",
    "    weight_init = tf.random_normal_initializer(\n",
    "        stddev=(2. / weight_shape[0]) ** 0.5)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable('W', weight_shape, initializer=weight_init)\n",
    "    b = tf.get_variable('b', bias_shape, initializer=bias_init)\n",
    "    return tf.nn.relu(tf.matmul(input, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(x, keep_prob):\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    \n",
    "    with tf.variable_scope('conv_1'):\n",
    "        conv_1 = conv2d(x, [5, 5, 1, 32], [32])\n",
    "        pool_1 = max_pool(conv_1)\n",
    "    with tf.variable_scope('conv_2'):\n",
    "        conv_2 = conv2d(pool_1, [5, 5, 32, 64], [64])\n",
    "        pool_2 = max_pool(conv_2)\n",
    "    with tf.variable_scope('fc'): # fully connected\n",
    "        pool_2_flat = tf.reshape(pool_2, [-1, 7 * 7 * 64])\n",
    "        fc_1 = layer(pool_2_flat, [7 * 7 *64, 1024], [1024])\n",
    "        \n",
    "        # apply dropout\n",
    "        fc_1_drop = tf.nn.dropout(fc_1, keep_prob)\n",
    "    with tf.variable_scope('output'):\n",
    "        output = layer(fc_1_drop, [1024, 10], [10])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(output, y):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, \n",
    "                                                       labels=y)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(cost, global_step):\n",
    "    tf.summary.scalar('cost', cost)\n",
    "    optimizer = tf.train.AdamOptimizer(ETA)\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(output, y):\n",
    "    correct_pred = tf.equal(tf.argmax(output, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    tf.summary.scalar('validation_error', (1. - accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001\tcost: 0.31770\tvalid. error: 0.02140\n",
      "Epoch: 0002\tcost: 0.08641\tvalid. error: 0.01640\n",
      "Epoch: 0003\tcost: 0.05942\tvalid. error: 0.01420\n",
      "Epoch: 0004\tcost: 0.04650\tvalid. error: 0.01140\n",
      "Epoch: 0005\tcost: 0.03796\tvalid. error: 0.01160\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d94ed166c538>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                     minibatch_x, minibatch_y = mnist.train.next_batch(\n\u001b[0;32m---> 30\u001b[0;31m                         BATCH)\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     sess.run(train_op, feed_dict={x: minibatch_x, \n",
      "\u001b[0;32m~/Learning/neuralNet/funDeep/input_data.py\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(self, batch_size, fake_data)\u001b[0m\n\u001b[1;32m    148\u001b[0m       \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m       \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0;31m# Start next epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.variable_scope('mnist_conv_model'):\n",
    "            x = tf.placeholder('float', [None, 784]) # 28 * 28\n",
    "            y = tf.placeholder('float', [None, 10])  # 10 digits\n",
    "            keep_prob = tf.placeholder(tf.float32)   # for dropout\n",
    "            output = inference(x, keep_prob)\n",
    "            cost = loss(output, y)\n",
    "            global_step = tf.Variable(\n",
    "                0, name='global_step', trainable=False)\n",
    "            train_op = training(cost, global_step)\n",
    "            eval_op = evaluate(output, y)\n",
    "            summary_op = tf.summary.merge_all()\n",
    "            saver = tf.train.Saver()\n",
    "            sess = tf.Session()\n",
    "            summary_writer = tf.summary.FileWriter(\n",
    "                'conv_mnist_logs', graph=sess.graph)\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            \n",
    "            sess.run(init_op)\n",
    "            \n",
    "            # Training\n",
    "            for epoch in range(EPOCHS):\n",
    "                avg_cost = 0.\n",
    "                total_batches = int(mnist.train.num_examples / BATCH)\n",
    "                \n",
    "                # Loop over batches\n",
    "                for batch in range(total_batches):\n",
    "                    minibatch_x, minibatch_y = mnist.train.next_batch(\n",
    "                        BATCH)\n",
    "\n",
    "                    sess.run(train_op, feed_dict={x: minibatch_x, \n",
    "                                                  y: minibatch_y, \n",
    "                                                  keep_prob: 0.5})\n",
    "                    avg_cost += (sess.run(cost, \n",
    "                                          feed_dict={x: minibatch_x,\n",
    "                                                     y: minibatch_y,\n",
    "                                                     keep_prob: 0.5}) /\n",
    "                                 total_batches)\n",
    "                if epoch % DISPLAY_STEP == 0:\n",
    "                    accuracy = sess.run(\n",
    "                        eval_op, \n",
    "                        feed_dict={x: mnist.validation.images,\n",
    "                                   y: mnist.validation.labels,\n",
    "                                   keep_prob: 1.})\n",
    "                    \n",
    "                    print('Epoch: %04d\\tcost: %.5f\\tvalid. error: %.5f' \n",
    "                          %(epoch + 1, avg_cost, 1 - accuracy))\n",
    "                    summary_str = sess.run(summary_op, \n",
    "                                           feed_dict={x: minibatch_x,\n",
    "                                                      y: minibatch_y,\n",
    "                                                      keep_prob: 0.5})\n",
    "                    summary_writer.add_summary(summary_str, \n",
    "                                               sess.run(global_step))\n",
    "                    \n",
    "            print('Optimization finished!')\n",
    "            accuracy = sess.run(eval_op, feed_dict={x: mnist.test.images,\n",
    "                                                    y: mnist.test.labels,\n",
    "                                                    keep_prob: 1.})\n",
    "            print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
