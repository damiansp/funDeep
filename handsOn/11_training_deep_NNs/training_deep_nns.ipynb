{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/ageron/handson-ml\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../bookGithubRepoURL.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tensorflow.contrib.layers import variance_scaling_initializer \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "### Implementation in TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "n_in      = 28 * 28 # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_out     = 10\n",
    "eta       = 0.01\n",
    "momentum  = 0.25 \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_in), name = 'X')\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "is_training = tf.placeholder(tf.bool, shape = (), name = 'is_training')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    he_init = variance_scaling_initializer()\n",
    "    my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                                  training = is_training,\n",
    "                                  momentum = 0.9) # formerly 'decay'\n",
    "    my_dense_layer = partial(tf.layers.dense, kernel_initializer = he_init)\n",
    "    \n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name = 'hidden1')\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name = 'hidden2')\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(\n",
    "        bn2, n_out, activation = None, name = 'outputs')\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(x_entropy, name = 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.MomentumOptimizer(eta, momentum)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "# saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 Train accuracy: 0.7150   Test accuracy: 0.7611\n",
      " 1 Train accuracy: 0.8350   Test accuracy: 0.8324\n",
      " 2 Train accuracy: 0.8600   Test accuracy: 0.8568\n",
      " 3 Train accuracy: 0.8800   Test accuracy: 0.8722\n",
      " 4 Train accuracy: 0.8850   Test accuracy: 0.8791\n",
      " 5 Train accuracy: 0.8650   Test accuracy: 0.8857\n",
      " 6 Train accuracy: 0.8500   Test accuracy: 0.8935\n",
      " 7 Train accuracy: 0.9050   Test accuracy: 0.8978\n",
      " 8 Train accuracy: 0.8950   Test accuracy: 0.9019\n",
      " 9 Train accuracy: 0.9300   Test accuracy: 0.9060\n",
      "10 Train accuracy: 0.9050   Test accuracy: 0.9090\n",
      "11 Train accuracy: 0.9200   Test accuracy: 0.9107\n",
      "12 Train accuracy: 0.9000   Test accuracy: 0.9146\n",
      "13 Train accuracy: 0.8800   Test accuracy: 0.9141\n",
      "14 Train accuracy: 0.9450   Test accuracy: 0.9189\n",
      "15 Train accuracy: 0.9000   Test accuracy: 0.9202\n",
      "16 Train accuracy: 0.9250   Test accuracy: 0.9228\n",
      "17 Train accuracy: 0.9050   Test accuracy: 0.9237\n",
      "18 Train accuracy: 0.9550   Test accuracy: 0.9251\n",
      "19 Train accuracy: 0.9200   Test accuracy: 0.9265\n",
      "20 Train accuracy: 0.9500   Test accuracy: 0.9269\n",
      "21 Train accuracy: 0.9250   Test accuracy: 0.9293\n",
      "22 Train accuracy: 0.9600   Test accuracy: 0.9288\n",
      "23 Train accuracy: 0.9450   Test accuracy: 0.9320\n",
      "24 Train accuracy: 0.9700   Test accuracy: 0.9329\n",
      "25 Train accuracy: 0.9500   Test accuracy: 0.9336\n",
      "26 Train accuracy: 0.9500   Test accuracy: 0.9340\n",
      "27 Train accuracy: 0.9350   Test accuracy: 0.9356\n",
      "28 Train accuracy: 0.9450   Test accuracy: 0.9368\n",
      "29 Train accuracy: 0.9350   Test accuracy: 0.9373\n",
      "30 Train accuracy: 0.9500   Test accuracy: 0.9384\n",
      "31 Train accuracy: 0.9400   Test accuracy: 0.9395\n",
      "32 Train accuracy: 0.9650   Test accuracy: 0.9414\n",
      "33 Train accuracy: 0.9250   Test accuracy: 0.9420\n",
      "34 Train accuracy: 0.9200   Test accuracy: 0.9411\n",
      "35 Train accuracy: 0.9600   Test accuracy: 0.9436\n",
      "36 Train accuracy: 0.9750   Test accuracy: 0.9438\n",
      "37 Train accuracy: 0.9650   Test accuracy: 0.9454\n",
      "38 Train accuracy: 0.9400   Test accuracy: 0.9449\n",
      "39 Train accuracy: 0.9600   Test accuracy: 0.9454\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as s:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(mnist.test.labels) // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            s.run(\n",
    "                [training_op, extra_update_ops], \n",
    "                feed_dict = { is_training: True, X: X_batch, y: y_batch })\n",
    "        acc_train = accuracy.eval(\n",
    "            feed_dict = { is_training: False, X: X_batch, y: y_batch })\n",
    "        acc_test = accuracy.eval(feed_dict = { is_training: False, \n",
    "                                               X: mnist.test.images, \n",
    "                                               y: mnist.test.labels })\n",
    "        print('%2d Train accuracy: %.4f   Test accuracy: %.4f'\n",
    "              %(epoch, acc_train, acc_test))\n",
    "        # save_path = saver.save(s, 'my_batch_norm_mod.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "To prevent exploding/vanishing gradients, simply clip the gradient to some threshold value (mostly useful for recurrent nns). \n",
    "NOTE: Batch normalization is generally preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 1.\n",
    "optimizer = tf.train.GradientDescentOptimizer(eta)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "# run this training_op at every training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers\n",
    "### Reusing a TF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "with tf.Session() as s:\n",
    "    saver.restore(s, './pretrained_model.ckpt')\n",
    "    # ...```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reuse only SOME layers, Saver must be configured accordingly\n",
    "\n",
    "```\n",
    "# <Build new model with same def as before for hidden layers 1 - 3>\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                               scope = 'hidden[123]')\n",
    "reuse_vars_dict = dict(\n",
    "    [(var.name, var.name) for var in reuse_vars])\n",
    "# saver to restore original mod\n",
    "original_saver = tf.Saver(reuse_vars) \n",
    "# ...and to save new mod\n",
    "new_saver = tf.Saver()                \n",
    "\n",
    "with tf.Session() as s:\n",
    "    s.run(init)\n",
    "    # restore layers 1-3\n",
    "    original_saver.restore('./pretrained_model.ckpt') \n",
    "    # <Train new mod>\n",
    "    new_saver.save('./new_model.ckpt') # save whole model```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate = eta, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(\n",
    "    learning_rate = eta, momentum = 0.9, use_nesterov = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "### RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(\n",
    "    learning_rate = eta, momentum = 0.9, decay = 0.9, epsilon = 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimization\n",
    "This is a combination of many of the above, and generally yields the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate = eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
