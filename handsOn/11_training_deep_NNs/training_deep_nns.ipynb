{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/ageron/handson-ml\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../bookGithubRepoURL.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tensorflow.contrib.layers import variance_scaling_initializer \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "### Implementation in TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "n_in      = 28 * 28 # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_out     = 10\n",
    "eta       = 0.01\n",
    "momentum  = 0.25 \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_in), name = 'X')\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "is_training = tf.placeholder(tf.bool, shape = (), name = 'is_training')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    he_init = variance_scaling_initializer()\n",
    "    my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                                  training = is_training,\n",
    "                                  momentum = 0.9) # formerly 'decay'\n",
    "    my_dense_layer = partial(tf.layers.dense, kernel_initializer = he_init)\n",
    "    \n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name = 'hidden1')\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name = 'hidden2')\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(\n",
    "        bn2, n_out, activation = None, name = 'outputs')\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(x_entropy, name = 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.MomentumOptimizer(eta, momentum)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "# saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 Train accuracy: 0.7700   Test accuracy: 0.7636\n",
      " 1 Train accuracy: 0.8000   Test accuracy: 0.8281\n",
      " 2 Train accuracy: 0.8600   Test accuracy: 0.8544\n",
      " 3 Train accuracy: 0.9100   Test accuracy: 0.8656\n",
      " 4 Train accuracy: 0.8650   Test accuracy: 0.8761\n",
      " 5 Train accuracy: 0.8500   Test accuracy: 0.8843\n",
      " 6 Train accuracy: 0.8750   Test accuracy: 0.8896\n",
      " 7 Train accuracy: 0.9100   Test accuracy: 0.8968\n",
      " 8 Train accuracy: 0.9050   Test accuracy: 0.9004\n",
      " 9 Train accuracy: 0.8950   Test accuracy: 0.9042\n",
      "10 Train accuracy: 0.8950   Test accuracy: 0.9064\n",
      "11 Train accuracy: 0.9100   Test accuracy: 0.9086\n",
      "12 Train accuracy: 0.9250   Test accuracy: 0.9119\n",
      "13 Train accuracy: 0.8950   Test accuracy: 0.9139\n",
      "14 Train accuracy: 0.9050   Test accuracy: 0.9173\n",
      "15 Train accuracy: 0.9150   Test accuracy: 0.9193\n",
      "16 Train accuracy: 0.9250   Test accuracy: 0.9219\n",
      "17 Train accuracy: 0.9200   Test accuracy: 0.9234\n",
      "18 Train accuracy: 0.9400   Test accuracy: 0.9248\n",
      "19 Train accuracy: 0.9100   Test accuracy: 0.9248\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as s:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(mnist.test.labels) // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            s.run(\n",
    "                [training_op, extra_update_ops], \n",
    "                feed_dict = { is_training: True, X: X_batch, y: y_batch })\n",
    "        acc_train = accuracy.eval(\n",
    "            feed_dict = { is_training: False, X: X_batch, y: y_batch })\n",
    "        acc_test = accuracy.eval(feed_dict = { is_training: False, \n",
    "                                               X: mnist.test.images, \n",
    "                                               y: mnist.test.labels })\n",
    "        print('%2d Train accuracy: %.4f   Test accuracy: %.4f'\n",
    "              %(epoch, acc_train, acc_test))\n",
    "        # save_path = saver.save(s, 'my_batch_norm_mod.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "To prevent exploding/vanishing gradients, simply clip the gradient to some threshold value (mostly useful for recurrent nns). \n",
    "NOTE: Batch normalization is generally preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.\n",
    "optimizer = tf.train.GradientDescentOptimizer(eta)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "# run this training_op at every training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers\n",
    "### Reusing a TF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "with tf.Session() as s:\n",
    "    saver.restore(s, './pretrained_model.ckpt')\n",
    "    # ...```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reuse only SOME layers, Saver must be configured accordingly\n",
    "\n",
    "```\n",
    "# <Build new model with same def as before for hidden layers 1 - 3>\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                               scope = 'hidden[123]')\n",
    "reuse_vars_dict = dict(\n",
    "    [(var.name, var.name) for var in reuse_vars])\n",
    "# saver to restore original mod\n",
    "original_saver = tf.Saver(reuse_vars) \n",
    "# ...and to save new mod\n",
    "new_saver = tf.Saver()                \n",
    "\n",
    "with tf.Session() as s:\n",
    "    s.run(init)\n",
    "    # restore layers 1-3\n",
    "    original_saver.restore('./pretrained_model.ckpt') \n",
    "    # <Train new mod>\n",
    "    new_saver.save('./new_model.ckpt') # save whole model```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
