{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://github.com/ageron/handson-ml\r\n"
     ]
    }
   ],
   "source": [
    "!cat ../bookGithubRepoURL.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tensorflow.contrib.layers import dropout, variance_scaling_initializer \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "### Implementation in TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\")\n",
    "n_in      = 28 * 28 # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_out     = 10\n",
    "eta       = 0.01\n",
    "momentum  = 0.25 \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = (None, n_in), name = 'X')\n",
    "y = tf.placeholder(tf.int64, shape = (None), name = 'y')\n",
    "is_training = tf.placeholder(tf.bool, shape = (), name = 'is_training')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    he_init = variance_scaling_initializer()\n",
    "    my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                                  training = is_training,\n",
    "                                  momentum = 0.9) # formerly 'decay'\n",
    "    my_dense_layer = partial(tf.layers.dense, kernel_initializer = he_init)\n",
    "    \n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name = 'hidden1')\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name = 'hidden2')\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(\n",
    "        bn2, n_out, activation = None, name = 'outputs')\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels = y, logits = logits)\n",
    "    loss = tf.reduce_mean(x_entropy, name = 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.MomentumOptimizer(eta, momentum)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "# saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 Train accuracy: 0.7750   Test accuracy: 0.7606\n",
      " 1 Train accuracy: 0.8150   Test accuracy: 0.8296\n",
      " 2 Train accuracy: 0.8850   Test accuracy: 0.8550\n",
      " 3 Train accuracy: 0.8900   Test accuracy: 0.8687\n",
      " 4 Train accuracy: 0.8900   Test accuracy: 0.8798\n",
      " 5 Train accuracy: 0.8450   Test accuracy: 0.8869\n",
      " 6 Train accuracy: 0.9100   Test accuracy: 0.8935\n",
      " 7 Train accuracy: 0.9000   Test accuracy: 0.8968\n",
      " 8 Train accuracy: 0.8750   Test accuracy: 0.9017\n",
      " 9 Train accuracy: 0.8750   Test accuracy: 0.9038\n",
      "10 Train accuracy: 0.9350   Test accuracy: 0.9098\n",
      "11 Train accuracy: 0.9000   Test accuracy: 0.9128\n",
      "12 Train accuracy: 0.9250   Test accuracy: 0.9147\n",
      "13 Train accuracy: 0.9400   Test accuracy: 0.9162\n",
      "14 Train accuracy: 0.9250   Test accuracy: 0.9185\n",
      "15 Train accuracy: 0.9000   Test accuracy: 0.9212\n",
      "16 Train accuracy: 0.9200   Test accuracy: 0.9230\n",
      "17 Train accuracy: 0.9300   Test accuracy: 0.9251\n",
      "18 Train accuracy: 0.9350   Test accuracy: 0.9270\n",
      "19 Train accuracy: 0.9450   Test accuracy: 0.9287\n",
      "20 Train accuracy: 0.9000   Test accuracy: 0.9303\n",
      "21 Train accuracy: 0.9350   Test accuracy: 0.9319\n",
      "22 Train accuracy: 0.9450   Test accuracy: 0.9334\n",
      "23 Train accuracy: 0.9550   Test accuracy: 0.9352\n",
      "24 Train accuracy: 0.9400   Test accuracy: 0.9350\n",
      "25 Train accuracy: 0.9700   Test accuracy: 0.9360\n",
      "26 Train accuracy: 0.9350   Test accuracy: 0.9368\n",
      "27 Train accuracy: 0.9500   Test accuracy: 0.9374\n",
      "28 Train accuracy: 0.9550   Test accuracy: 0.9401\n",
      "29 Train accuracy: 0.9600   Test accuracy: 0.9398\n",
      "30 Train accuracy: 0.9650   Test accuracy: 0.9412\n",
      "31 Train accuracy: 0.9400   Test accuracy: 0.9420\n",
      "32 Train accuracy: 0.9250   Test accuracy: 0.9434\n",
      "33 Train accuracy: 0.9450   Test accuracy: 0.9443\n",
      "34 Train accuracy: 0.9400   Test accuracy: 0.9454\n",
      "35 Train accuracy: 0.9650   Test accuracy: 0.9461\n",
      "36 Train accuracy: 0.9550   Test accuracy: 0.9466\n",
      "37 Train accuracy: 0.9500   Test accuracy: 0.9473\n",
      "38 Train accuracy: 0.9350   Test accuracy: 0.9497\n",
      "39 Train accuracy: 0.9800   Test accuracy: 0.9488\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as s:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(mnist.test.labels) // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            s.run(\n",
    "                [training_op, extra_update_ops], \n",
    "                feed_dict = { is_training: True, X: X_batch, y: y_batch })\n",
    "        acc_train = accuracy.eval(\n",
    "            feed_dict = { is_training: False, X: X_batch, y: y_batch })\n",
    "        acc_test = accuracy.eval(feed_dict = { is_training: False, \n",
    "                                               X: mnist.test.images, \n",
    "                                               y: mnist.test.labels })\n",
    "        print('%2d Train accuracy: %.4f   Test accuracy: %.4f'\n",
    "              %(epoch, acc_train, acc_test))\n",
    "        # save_path = saver.save(s, 'my_batch_norm_mod.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "To prevent exploding/vanishing gradients, simply clip the gradient to some threshold value (mostly useful for recurrent nns). \n",
    "NOTE: Batch normalization is generally preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 1.\n",
    "optimizer = tf.train.GradientDescentOptimizer(eta)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "# run this training_op at every training step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers\n",
    "### Reusing a TF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "with tf.Session() as s:\n",
    "    saver.restore(s, './pretrained_model.ckpt')\n",
    "    # ...```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reuse only SOME layers, Saver must be configured accordingly\n",
    "\n",
    "```\n",
    "# <Build new model with same def as before for hidden layers 1 - 3>\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                               scope = 'hidden[123]')\n",
    "reuse_vars_dict = dict(\n",
    "    [(var.name, var.name) for var in reuse_vars])\n",
    "# saver to restore original mod\n",
    "original_saver = tf.Saver(reuse_vars) \n",
    "# ...and to save new mod\n",
    "new_saver = tf.Saver()                \n",
    "\n",
    "with tf.Session() as s:\n",
    "    s.run(init)\n",
    "    # restore layers 1-3\n",
    "    original_saver.restore('./pretrained_model.ckpt') \n",
    "    # <Train new mod>\n",
    "    new_saver.save('./new_model.ckpt') # save whole model```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate = eta, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(\n",
    "    learning_rate = eta, momentum = 0.9, use_nesterov = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "### RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(\n",
    "    learning_rate = eta, momentum = 0.9, decay = 0.9, epsilon = 1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam Optimization\n",
    "This is a combination of many of the above, and generally yields the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate = eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# e.g.\n",
    "initial_learning_rate = 0.1\n",
    "decay_steps = 10000\n",
    "decay_rate = 1/10\n",
    "global_step = tf.Variable(0, trainable = False)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    initial_learning_rate, global_step, decay_steps, decay_rate)\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum = 0.9)\n",
    "training_op = optimizer.minimize(loss, global_step = global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization to Avoid Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _L_<sub>1</sub> and _L_<sub>2</sub> Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ...constuct a network w/ say, one hidden layer with weights W1, and one \n",
    "# output layer with W2\n",
    "base_loss = tf.reduce_mean(xentropy, name='avg_xentropy')\n",
    "reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "loss = tf.add(base_loss, scale * reg_losses, name = 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# But for applying to many layers, the following is more efficient\n",
    "with arg_scope(\n",
    "    [fully_connected],\n",
    "    weights_regularizer = tf.contrib.layers.l1_regularizer(scale=0.01)):\n",
    "    \n",
    "    hidden1 = fully_connected(X, n_hidden1, scope='hidden1')\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, scope='hidden2')\n",
    "    logits = fully_connected(\n",
    "        hidden2, n_outputs, activation_fn=None, scope='out')\n",
    "    \n",
    "# Then add to loss function\n",
    "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.add_n([base_loss] + reg_losses, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout \n",
    "Note: if overfitting, increase the dropout rate; if underfitting, decrease the dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#...\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "keep_prob = 0.5\n",
    "\n",
    "X_drop = dropout(X, keep_prob, is_training=is_training)\n",
    "\n",
    "hidden1 = fully_connected(X_drop, n_hidden1, scope='hidden1')\n",
    "hidden1_drop = dropout(hidden1, keep_prob, is_training=is_training)\n",
    "\n",
    "hidden2 = fully_connected(hidden1_drop, n_hidden2, scope='hidden2')\n",
    "hidden2_drop = dropout(hidden2, keep_prob, is_training=is_training)\n",
    "\n",
    "logits = fully_connected(\n",
    "    hidden2_drop, n_outputs, activation_fn=None, scope='outputs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 1. # max norm for w, the vector of incoming weights to a neuron\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)\n",
    "\n",
    "# Apply at each step\n",
    "with tf.Session() as s:\n",
    "    #...\n",
    "    for epoch in range(n_epochs):\n",
    "        #...\n",
    "        for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "            s.run(training_op, feed_dict={ X: X_batch, y: y_batch })\n",
    "            clip_weights.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to get access to the weights at each layer\n",
    "hidden1 = fully_connected(X, n_hidden1, scope='hidden1')\n",
    "with tf.variable_scope('hidden1', reuse=True):\n",
    "    weights1 = tf.get_variable('weights')\n",
    "    \n",
    "# OR\n",
    "hidden1 = fully_connected(X, n_hidden1, scope='hidden1')\n",
    "hidden2 = fully_connected(hidden1, n_hidden2, scope='hidden2')\n",
    "#...\n",
    "\n",
    "with tf.variable_scope('', default_name='', reuse=True): # root scope\n",
    "    w1 = tf.get_variable('hidden1/weights')\n",
    "    w2 = tf.get_variable('hidden2/weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see a list of variables\n",
    "for var in tf.global_variables():\n",
    "    print(var.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaner solution\n",
    "def max_norm_regularizer(\n",
    "    threshold, axes=1, name='max_norm', collection='max_norm'):\n",
    "    \n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # no regularization loss term\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Can be used as...\n",
    "max_norm_reg = max_norm_regularizer(threshold=1.)\n",
    "hidden1 = fully_connected(\n",
    "    X, n_hidden1, scope='hidden1', weights_regularizer=max_norm_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clipping still has to be done after each step\n",
    "clip_all_weights = tf.get_collection('max_norm')\n",
    "\n",
    "with tf.Session() as s:\n",
    "    # ...\n",
    "    for epoch in range(n_epochs):\n",
    "        # ...\n",
    "        for X_batch, y_batch in zip(X_batches, y_batches):\n",
    "            s.run(training_op, feed_dict={ X: X_batch, y: y_batch })\n",
    "            s.run(clip_all_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
